[![Python Version](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?logo=opencv&logoColor=white)](https://opencv.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-007BFF?logo=mediapipe&logoColor=white)](https://mediapipe.dev/)
[![Tkinter](https://img.shields.io/badge/Tkinter-GUI-orange)](https://docs.python.org/3/library/tkinter.html)
[![Godot Engine](https://img.shields.io/badge/Godot-Engine-478CB0?logo=godotengine&logoColor=white)](https://godotengine.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

<p align="center">
  <img src=".github/logo.png" width="200px">
</p>

<h1 align="center">STRG - Sistema de TraduÃ§Ã£o e Reconhecimento de Gestos</h1>

<p align="center">
  <em>Um olhar inovador sobre a comunicaÃ§Ã£o gestual, potenciado por inteligÃªncia artificial.</em>
</p>

<p align="center">
  <strong>Projeto PAP 2025</strong>
</p>

<p align="center">
  <img src=".github/demo.gif" height="380px">
</p>

## ğŸ¯ Sobre o Projeto

### O STRG Ã© uma soluÃ§Ã£o desenvolvida em python que visa desmistificar e traduzir linguagem gestual em tempo real. Mais do que um simples tradutor, o projeto explora funcionalidades adicionais para enriquecer a interaÃ§Ã£o e oferecer uma ferramenta intuitiva. Este projeto nasceu da vontade de explorar o potencial da visÃ£o computacional e machine learning.

## âœ¨ Funcionalidades Detalhadas por MÃ³dulo

O STRG Ã© composto por vÃ¡rios mÃ³dulos, cada um com funcionalidades especÃ­ficas, acessÃ­veis atravÃ©s do launcher principal (`STRG.py`):

1.  **Reconhecimento de Gestos (F1)**

    - Script: `src/main/main.py`
    - DescriÃ§Ã£o: Interpreta gestos manuais capturados pela cÃ¢mara em tempo real. Utiliza um modelo de reconhecimento de gestos prÃ³prio.

2.  **Reconhecimento de Palavras (F2)**

    - Script: `src/main/word_recognition/word_recognition_app.py`
    - DescriÃ§Ã£o: AplicaÃ§Ã£o dedicada ao reconhecimento de palavras.

3.  **Controlo do Cursor (F3)**

    - Script: `src/utils/mouse-control-hand/mouse_control.py`
    - DescriÃ§Ã£o: Permite controlar o cursor do rato no ecrÃ£ utilizando os gestos da mÃ£o detetados pela cÃ¢mara.

4.  **Controlo do Volume (F4)**

    - Script: `src/utils/volume-control-hand/main.py`
    - DescriÃ§Ã£o: Ajusta o volume do sistema operativo atravÃ©s de gestos especÃ­ficos da mÃ£o.

5.  **Reconhecimento Facial (F5)**

    - Script: `src/utils/face-recon/face.py`
    - DescriÃ§Ã£o: Identifica ou verifica faces com base numa base de dados local.

6.  **VisÃ£o BinÃ¡ria (F6)**

    - Script: `src/utils/binary-vision/binary_vision.py`
    - DescriÃ§Ã£o: Aplica um threshold Ã  imagem da cÃ¢mara para criar uma visualizaÃ§Ã£o a preto e branco (binÃ¡ria).

7.  **Visualizador 3D da MÃ£o (F7)**

    - Script: `src/utils/3d-hand-viewer/python/hand_detection.py`
    - DescriÃ§Ã£o: Deteta a mÃ£o e os seus pontos de referÃªncia (landmarks) e Ã© usado para renderizar uma representaÃ§Ã£o 3D da mÃ£o. Este script Python interage com uma game engine chamada Godot.

8.  **Menu de Performance (F8)**

    - Script: `src/utils/menus/performance-menu/performance_menu.py`
    - DescriÃ§Ã£o: Apresenta um menu para monitorizar ou ajustar parÃ¢metros relacionados com a performance das aplicaÃ§Ãµes.

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.10:** Linguagem principal de desenvolvimento.
- **Tkinter:** Para a interface grÃ¡fica do menu principal (`STRG.py`).
- **OpenCV:** Para processamento de imagem, captura de vÃ­deo e funcionalidades de visÃ£o computacional em vÃ¡rios mÃ³dulos.
- **MediaPipe:** Para deteÃ§Ã£o de mÃ£os, rosto e landmarks de forma eficiente, utilizado em mÃ³dulos como o `HandTrackingModule.py` e noutros mÃ³dulos de reconhecimento.
- **TensorFlow/Keras:** Para a construÃ§Ã£o, treino e utilizaÃ§Ã£o de modelos de Machine Learning (evidenciado pela presenÃ§a de ficheiros de modelo e pastas `models/`).
- **Pynput:** Para escuta de eventos de teclado globais (atalhos F1-F8 no launcher).
- **Subprocess:** Para lanÃ§ar os diferentes mÃ³dulos em terminais separados.
- **Outras bibliotecas (implÃ­citas):** `os`, `shutil`, `random`, `math`, e `pickle` para serializaÃ§Ã£o de dados (ex: `face_database.pkl`).

## ğŸš€ Como ComeÃ§ar

Para pÃ´r o STRG a funcionar no teu sistema, segue estes passos.

### ğŸ“‹ PrÃ©-requisitos

- Python 3.10.

### ğŸ› ï¸ InstalaÃ§Ã£o de DependÃªncias do Sistema

Abre o teu terminal e executa os seguintes comandos para instalar as dependÃªncias essenciais.

1.  **Ferramentas de Desenvolvimento Python 3.10 (Exemplo para Fedora):**
    ```bash
    sudo dnf install -y python3.10-devel
    ```
    Para outras distribuiÃ§Ãµes linux (ex: baseadas em debian/ubuntu), o comando seria algo como:
    ```
    sudo apt-get update
    sudo apt-get install -y python3.10-dev
    ```
2.  **Outras DependÃªncias CrÃ­ticas (Exemplo para Fedora):**
    Isto inclui bibliotecas essenciais para desenvolvimento Python, interface grÃ¡fica X11, emuladores de terminal (para lanÃ§ar os mÃ³dulos), `wmctrl` (para gestÃ£o de janelas), e o Godot Engine (se pretenderes explorar a visualizaÃ§Ã£o 3D).

    ```bash
    sudo dnf install -y python3-devel libX11-devel libXtst-devel gnome-terminal xterm wmctrl godot
    ```

    Para outras distribuiÃ§Ãµes (ex: debian/ubuntu):

    ```
    sudo apt-get install -y python3-dev libx11-dev libxtst-dev gnome-terminal xterm wmctrl godot
    ```

### âš™ï¸ ConfiguraÃ§Ã£o do Ambiente de Desenvolvimento Python

ApÃ³s a instalaÃ§Ã£o das dependÃªncias do sistema, Ã© crucial configurar um ambiente virtual Python isolado para o projeto.

1.  **Clona o RepositÃ³rio e Entra no DiretÃ³rio:**

    ```bash
    git clone https://github.com/jdias2019/STRG.git
    cd STRG
    ```

2.  **Cria o Ambiente Virtual:**
    Recomenda-se usar o nome `venv_STRG` ou `venv_py310` como visto na estrutura do projeto.

    ```bash
    python3.10 -m venv venv_STRG
    ```

3.  **Ativa o Ambiente Virtual:**
    Este passo Ã© necessÃ¡rio sempre que quiseres trabalhar no projeto.

    ```bash
    source venv_STRG/bin/activate
    ```

    O teu prompt do terminal deverÃ¡ mudar, indicando que o ambiente virtual estÃ¡ ativo.

4.  **Atualiza o Pip:**
    Garante que tens a versÃ£o mais recente do `pip`.

    ```bash
    pip install --upgrade pip
    ```

5.  **Instala as DependÃªncias do Projeto:**
    Este comando instala todas as bibliotecas Python no ficheiro `requirements.txt`.
    ```bash
    pip install -r requirements.txt
    ```

### â–¶ï¸ Executar o STRG (GUI)

Com o ambiente configurado e ativo, podes iniciar o menu principal do STRG:

1.  **Certifica-te que o ambiente virtual estÃ¡ ativo:**
    ```bash
    source venv_STRG/bin/activate
    ```
2.  **Executa o script do launcher:**
    ```bash
    python STRG.py
    ```
    Isto abrirÃ¡ uma GUI, a partir da qual poderÃ¡s lanÃ§ar os diferentes mÃ³dulos clicando nos botÃµes correspondentes ou usando as teclas de funÃ§Ã£o (F1-F8). Cada mÃ³dulo serÃ¡, por norma, aberto numa nova janela de terminal.

### ğŸ¤– Como Usar os Diferentes MÃ³dulos

ApÃ³s lanÃ§ar `STRG.py`, a interface principal permitir-te-Ã¡ iniciar cada um dos mÃ³dulos descritos na secÃ§Ã£o "Funcionalidades Detalhadas por MÃ³dulo".

- **Visualizador 3D (Godot):** Para a componente de visualizaÃ§Ã£o 3D da mÃ£o, o script Python (`src/utils/3d-hand-viewer/python/hand_detection.py`) Ã© o que Ã© lanÃ§ado pelo menu. Existe um projeto Godot complementar, terÃ¡s de o abrir separadamente no Godot Engine:
  1.  Abre o Godot Engine.
  2.  No gestor de projetos, clica em "Importar".
  3.  Navega atÃ© Ã  pasta do projeto Godot (ex: `src/utils/3d-hand-viewer/godot/`) e seleciona o ficheiro `project.godot`.
  4.  Abre o projeto e pressiona F5 para o executar.

### ğŸ§  Como Treinar os Modelos

O treino de modelos de Machine Learning Ã© especÃ­fico para certos mÃ³dulos do STRG. Segue os guias detalhados para cada tipo de modelo:

#### ğŸ“‹ 1. Reconhecimento de Gestos (`src/main/main.py`)

Este mÃ³dulo permite treinar um modelo personalizado para reconhecer gestos manuais especÃ­ficos.

**Passo 1: Colectar Amostras**

1. Executa o mÃ³dulo principal de gestos atravÃ©s do launcher ou diretamente:
   ```bash
   python src/main/main.py
   ```
2. No menu, seleciona a opÃ§Ã£o **"1 - Coletar Amostras"**
3. Define o nome do gesto que queres treinar (ex: "ola", "tchau", "ok")
4. Posiciona a tua mÃ£o na frente da cÃ¢mara
5. Pressiona **ESPAÃ‡O** para comeÃ§ar a gravaÃ§Ã£o de uma amostra
6. Executa o gesto durante a gravaÃ§Ã£o (30 frames por amostra)
7. Repete o processo vÃ¡rias vezes para o mesmo gesto (recomenda-se pelo menos 50-100 amostras por gesto)
8. Repete para diferentes gestos que queres que o modelo reconheÃ§a

**Passo 2: Treinar o Modelo**

1. No menu principal, seleciona **"2 - Treinar Modelo"**
2. O sistema irÃ¡:
   - Carregar todos os dados recolhidos do diretÃ³rio `dataset/`
   - Dividir os dados em treino (80%) e teste (20%)
   - Treinar uma rede neural com arquitetura Dense + Dropout
   - Utilizar Early Stopping para evitar overfitting
   - Guardar o modelo em `models/custom/custom_gesture_model.keras`
   - Gerar um grÃ¡fico do histÃ³rico de treino

**Requisitos:**

- MÃ­nimo de 2 gestos diferentes
- Pelo menos 50 amostras por gesto para resultados adequados
- Cada amostra deve ter exatamente 30 frames

#### ğŸ“ 2. Reconhecimento de Palavras (`src/main/word_recognition/`)

Este mÃ³dulo treina um modelo LSTM para reconhecer sequÃªncias de palavras em linguagem gestual.

**Passo 1: Colectar SequÃªncias de Palavras**

1. Executa a aplicaÃ§Ã£o de reconhecimento de palavras:
   ```bash
   python src/main/word_recognition/word_recognition_app.py
   ```
2. Seleciona **"1 - Coletar SequÃªncias"**
3. Escolhe uma palavra da lista ou cria uma nova
4. Posiciona-te na frente da cÃ¢mara
5. Pressiona **ESPAÃ‡O** para comeÃ§ar a gravar uma sequÃªncia
6. Executa os gestos que formam a palavra completa (30 frames por sequÃªncia)
7. Coleciona mÃºltiplas amostras da mesma palavra (mÃ­nimo 2, recomenda-se 20-30)
8. Repete para diferentes palavras

**Passo 2: Treinar o Modelo LSTM**

1. No menu principal, seleciona **"2 - Treinar Modelo"**
2. O sistema irÃ¡:
   - Carregar sequÃªncias de `src/main/word_recognition/data/`
   - Verificar se hÃ¡ amostras suficientes por palavra (mÃ­nimo 2)
   - Treinar um modelo LSTM sequencial com 3 camadas
   - Utilizar divisÃ£o estratificada (80% treino, 20% teste)
   - Aplicar callbacks de Early Stopping
   - Guardar o modelo em `src/main/word_recognition/models/`
   - Gerar relatÃ³rio de classificaÃ§Ã£o e mÃ©tricas

#### ğŸ‘¤ 3. Reconhecimento Facial (`src/utils/face-recon/`)

Este mÃ³dulo permite treinar o sistema para reconhecer faces especÃ­ficas utilizando features extraÃ­das por MobileNetV2.

**MÃ©todo de Treino (Captura em Tempo Real):**

1. Executa o mÃ³dulo de reconhecimento facial:
   ```bash
   python src/utils/face-recon/face.py
   ```
2. Pressiona **'t'** para ativar o modo de treino
3. Introduz o nome da pessoa que queres registar
4. Posiciona a face na Ã¡rea de deteÃ§Ã£o (retÃ¢ngulo verde)
5. Pressiona **'c'** para capturar e guardar a face
6. O sistema irÃ¡:
   - Detetar a face usando MediaPipe
   - Extrair a regiÃ£o facial
   - Redimensionar para 224Ã—224 pixels
   - Extrair features usando MobileNetV2 prÃ©-treinada
   - Guardar as features em `known_faces_data.npz`
   - Guardar imagem de referÃªncia em `ref_faces/`

### ğŸ”§ Models

- Gestos: `models/custom/custom_gesture_model.keras`
- Palavras: `src/main/word_recognition/models/word_recognition_model.keras`
- Faces: `models/custom/known_faces_data.npz`

### ğŸ“ Estrutura do Projeto

```
STRG/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ # Assets para o GitHub (logo, demo.gif).
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ main.py                     # Reconhecimento de Gestos.
â”‚   â”‚   â”œâ”€â”€ gesture_recognizer.task     # Modelo prÃ©-treinado MediaPipe.
â”‚   â”‚   â””â”€â”€ word_recognition/
â”‚   â”‚       â””â”€â”€ word_recognition_app.py # Reconhecimento de Palavras.
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ HandTrackingModule.py       # MÃ³dulo partilhado para deteÃ§Ã£o de mÃ£os.
â”‚       â”œâ”€â”€ 3d-hand-viewer/
â”‚       â”‚   â””â”€â”€ python/
â”‚       â”‚       â””â”€â”€ hand_detection.py   # Visualizador 3D da mÃ£o.
â”‚       â”œâ”€â”€ binary-vision/
â”‚       â”‚   â””â”€â”€ binary_vision.py        # VisÃ£o binÃ¡ria.
â”‚       â”œâ”€â”€ face-recon/
â”‚       â”‚   â””â”€â”€ face.py                 # Reconhecimento facial.
â”‚       â”œâ”€â”€ menus/
â”‚       â”‚   â””â”€â”€ performance-menu/
â”‚       â”‚       â””â”€â”€ performance_menu.py # Menu de performance.
â”‚       â”œâ”€â”€ mouse-control-hand/
â”‚       â”‚   â””â”€â”€ mouse_control.py        # Controlo do cursor.
â”‚       â””â”€â”€ volume-control-hand/
â”‚           â””â”€â”€ main.py                 # Controlo do volume.
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ LSTM-explicada.html     # VisualizaÃ§Ã£o interativa do modelo LSTM.
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt        # DependÃªncias Python.
â””â”€â”€ STRG.py                 # Launcher principal.
```

## ğŸ™ Agradecimentos

- O desenvolvimento da componente de visualizaÃ§Ã£o 3D da mÃ£o foi conceptualmente inspirado pelo trabalho de Florian Rival no projeto [virtual-hand-clone](https://github.com/trflorian/virtual-hand-clone).
- A visualizaÃ§Ã£o LSTM foi inspirada nos artigos [Neural Networks: Representation](https://www.jeremyjordan.me/intro-to-neural-networks/) de Jeremy Jordan e [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.<br>

- Outras inspiraÃ§Ãµes: <br>
  - [hand-gesture-recognition-mediapipe by kinivi](https://github.com/kinivi/hand-gesture-recognition-mediapipe); <br>
  - [colah.github.io](https://colah.github.io/);

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob os termos da [LicenÃ§a MIT](LICENSE).
