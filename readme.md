[![Python Version](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?logo=opencv&logoColor=white)](https://opencv.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-007BFF?logo=mediapipe&logoColor=white)](https://mediapipe.dev/)
[![Tkinter](https://img.shields.io/badge/Tkinter-GUI-orange)](https://docs.python.org/3/library/tkinter.html)
[![Godot Engine](https://img.shields.io/badge/Godot-Engine-478CB0?logo=godotengine&logoColor=white)](https://godotengine.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

<p align="center">
  <img src=".github/logo.png" width="200px">
</p>

<h1 align="center">STRG - Sistema de Tradu√ß√£o e Reconhecimento de Gestos</h1>

<p align="center">
  <em>Um olhar inovador sobre a comunica√ß√£o gestual, potenciado por intelig√™ncia artificial.</em>
</p>

<p align="center">
  <strong>Projeto PAP 2025</strong>
</p>

<p align="center">
  <img src=".github/demo.gif" height="380px">
</p>

## üéØ Sobre o Projeto

### O STRG √© uma solu√ß√£o desenvolvida em python que visa desmistificar e traduzir linguagem gestual em tempo real. Mais do que um simples tradutor, o projeto explora funcionalidades adicionais para enriquecer a intera√ß√£o e oferecer uma ferramenta intuitiva. Este projeto nasceu da vontade de explorar o potencial da vis√£o computacional e machine learning.

## ‚ú® Funcionalidades Detalhadas por M√≥dulo

O STRG √© composto por v√°rios m√≥dulos, cada um com funcionalidades espec√≠ficas, acess√≠veis atrav√©s do launcher principal (`STRG.py`):

1.  **Reconhecimento de Gestos (F1)**

    - Script: `src/main/main.py`
    - Descri√ß√£o: Interpreta gestos manuais capturados pela c√¢mara em tempo real. Utiliza um modelo de reconhecimento de gestos pr√≥prio.

2.  **Reconhecimento de Palavras (F2)**

    - Script: `src/main/word_recognition/word_recognition_app.py`
    - Descri√ß√£o: Aplica√ß√£o dedicada ao reconhecimento de palavras.

3.  **Controlo do Cursor (F3)**

    - Script: `src/utils/mouse-control-hand/mouse_control.py`
    - Descri√ß√£o: Permite controlar o cursor do rato no ecr√£ utilizando os gestos da m√£o detetados pela c√¢mara.

4.  **Controlo do Volume (F4)**

    - Script: `src/utils/volume-control-hand/main.py`
    - Descri√ß√£o: Ajusta o volume do sistema operativo atrav√©s de gestos espec√≠ficos da m√£o.

5.  **Reconhecimento Facial (F5)**

    - Script: `src/utils/face-recon/face.py`
    - Descri√ß√£o: Identifica ou verifica faces com base numa base de dados local.

6.  **Vis√£o Bin√°ria (F6)**

    - Script: `src/utils/binary-vision/binary_vision.py`
    - Descri√ß√£o: Aplica um threshold √† imagem da c√¢mara para criar uma visualiza√ß√£o a preto e branco (bin√°ria).

7.  **Visualizador 3D da M√£o (F7)**

    - Script: `src/utils/3d-hand-viewer/python/hand_detection.py`
    - Descri√ß√£o: Deteta a m√£o e os seus pontos de refer√™ncia (landmarks) e √© usado para renderizar uma representa√ß√£o 3D da m√£o. Este script Python interage com uma game engine chamada Godot.

8.  **Menu de Performance (F8)**

    - Script: `src/utils/menus/performance-menu/performance_menu.py`
    - Descri√ß√£o: Apresenta um menu para monitorizar ou ajustar par√¢metros relacionados com a performance das aplica√ß√µes.

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.10:** Linguagem principal de desenvolvimento.
- **Tkinter:** Para a interface gr√°fica do menu principal (`STRG.py`).
- **OpenCV:** Para processamento de imagem, captura de v√≠deo e funcionalidades de vis√£o computacional em v√°rios m√≥dulos.
- **MediaPipe:** Para dete√ß√£o de m√£os, rosto e landmarks de forma eficiente, utilizado em m√≥dulos como o `HandTrackingModule.py` e noutros m√≥dulos de reconhecimento.
- **TensorFlow/Keras:** Para a constru√ß√£o, treino e utiliza√ß√£o de modelos de Machine Learning (evidenciado pela presen√ßa de ficheiros de modelo e pastas `models/`).
- **Pynput:** Para escuta de eventos de teclado globais (atalhos F1-F8 no launcher).
- **Subprocess:** Para lan√ßar os diferentes m√≥dulos em terminais separados.
- **Outras bibliotecas (impl√≠citas):** `os`, `shutil`, `random`, `math`, e `pickle` para serializa√ß√£o de dados (ex: `face_database.pkl`).

## üöÄ Como Come√ßar

Para p√¥r o STRG a funcionar no teu sistema, segue estes passos.

### üìã Pr√©-requisitos

- Python 3.10.

### üõ†Ô∏è Instala√ß√£o de Depend√™ncias do Sistema

Abre o teu terminal e executa os seguintes comandos para instalar as depend√™ncias essenciais.

1.  **Ferramentas de Desenvolvimento Python 3.10 (Exemplo para Fedora):**
    ```bash
    sudo dnf install -y python3.10-devel
    ```
    Para outras distribui√ß√µes linux (ex: baseadas em debian/ubuntu), o comando seria algo como:
    ```
    sudo apt-get update
    sudo apt-get install -y python3.10-dev
    ```
2.  **Outras Depend√™ncias Cr√≠ticas (Exemplo para Fedora):**
    Isto inclui bibliotecas essenciais para desenvolvimento Python, interface gr√°fica X11, emuladores de terminal (para lan√ßar os m√≥dulos), `wmctrl` (para gest√£o de janelas), e o Godot Engine (se pretenderes explorar a visualiza√ß√£o 3D).

    ```bash
    sudo dnf install -y python3-devel libX11-devel libXtst-devel gnome-terminal xterm wmctrl godot
    ```

    Para outras distribui√ß√µes (ex: debian/ubuntu):

    ```
    sudo apt-get install -y python3-dev libx11-dev libxtst-dev gnome-terminal xterm wmctrl godot
    ```

### ‚öôÔ∏è Configura√ß√£o do Ambiente de Desenvolvimento Python

Ap√≥s a instala√ß√£o das depend√™ncias do sistema, √© crucial configurar um ambiente virtual Python isolado para o projeto.

1.  **Clona o Reposit√≥rio e Entra no Diret√≥rio:**

    ```bash
    git clone https://github.com/jdias2019/STRG.git
    cd STRG
    ```

2.  **Cria o Ambiente Virtual:**
    Recomenda-se usar o nome `venv_STRG` ou `venv_py310` como visto na estrutura do projeto.

    ```bash
    python3.10 -m venv venv_STRG
    ```

3.  **Ativa o Ambiente Virtual:**
    Este passo √© necess√°rio sempre que quiseres trabalhar no projeto.

    ```bash
    source venv_STRG/bin/activate
    ```

    O teu prompt do terminal dever√° mudar, indicando que o ambiente virtual est√° ativo.

4.  **Atualiza o Pip:**
    Garante que tens a vers√£o mais recente do `pip`.

    ```bash
    pip install --upgrade pip
    ```

5.  **Instala as Depend√™ncias do Projeto:**
    Este comando instala todas as bibliotecas Python no ficheiro `requirements.txt`.
    ```bash
    pip install -r requirements.txt
    ```

### ‚ñ∂Ô∏è Executar o STRG (GUI)

Com o ambiente configurado e ativo, podes iniciar o menu principal do STRG:

1.  **Certifica-te que o ambiente virtual est√° ativo:**
    ```bash
    source venv_STRG/bin/activate
    ```
2.  **Executa o script do launcher:**
    ```bash
    python STRG.py
    ```
    Isto abrir√° uma GUI, a partir da qual poder√°s lan√ßar os diferentes m√≥dulos clicando nos bot√µes correspondentes ou usando as teclas de fun√ß√£o (F1-F8). Cada m√≥dulo ser√°, por norma, aberto numa nova janela de terminal.

### ü§ñ Como Usar os Diferentes M√≥dulos

Ap√≥s lan√ßar `STRG.py`, a interface principal permitir-te-√° iniciar cada um dos m√≥dulos descritos na sec√ß√£o "Funcionalidades Detalhadas por M√≥dulo".

- **Visualizador 3D (Godot):** Para a componente de visualiza√ß√£o 3D da m√£o, o script Python (`src/utils/3d-hand-viewer/python/hand_detection.py`) √© o que √© lan√ßado pelo menu. Existe um projeto Godot complementar, ter√°s de o abrir separadamente no Godot Engine:
  1.  Abre o Godot Engine.
  2.  No gestor de projetos, clica em "Importar".
  3.  Navega at√© √† pasta do projeto Godot (ex: `src/utils/3d-hand-viewer/godot/`) e seleciona o ficheiro `project.godot`.
  4.  Abre o projeto e pressiona F5 para o executar.

### üß† Como Treinar os Modelos

O treino de modelos de Machine Learning √© espec√≠fico para certos m√≥dulos do STRG. Segue os guias detalhados para cada tipo de modelo:

#### üìã 1. Reconhecimento de Gestos (`src/main/main.py`)

Este m√≥dulo permite treinar um modelo personalizado para reconhecer gestos manuais espec√≠ficos.

**Passo 1: Colectar Amostras**

1. Executa o m√≥dulo principal de gestos atrav√©s do launcher ou diretamente:
   ```bash
   python src/main/main.py
   ```
2. No menu, seleciona a op√ß√£o **"1 - Coletar Amostras"**
3. Define o nome do gesto que queres treinar (ex: "ola", "tchau", "ok")
4. Posiciona a tua m√£o na frente da c√¢mara
5. Pressiona **ESPA√áO** para come√ßar a grava√ß√£o de uma amostra
6. Executa o gesto durante a grava√ß√£o (30 frames por amostra)
7. Repete o processo v√°rias vezes para o mesmo gesto (recomenda-se pelo menos 50-100 amostras por gesto)
8. Repete para diferentes gestos que queres que o modelo reconhe√ßa

**Passo 2: Treinar o Modelo**

1. No menu principal, seleciona **"2 - Treinar Modelo"**
2. O sistema ir√°:
   - Carregar todos os dados recolhidos do diret√≥rio `dataset/`
   - Dividir os dados em treino (80%) e teste (20%)
   - Treinar uma rede neural com arquitetura Dense + Dropout
   - Utilizar Early Stopping para evitar overfitting
   - Guardar o modelo em `models/custom/custom_gesture_model.keras`
   - Gerar um gr√°fico do hist√≥rico de treino

**Requisitos:**

- M√≠nimo de 2 gestos diferentes
- Pelo menos 50 amostras por gesto para resultados adequados
- Cada amostra deve ter exatamente 30 frames

#### üìù 2. Reconhecimento de Palavras (`src/main/word_recognition/`)

Este m√≥dulo treina um modelo LSTM para reconhecer sequ√™ncias de palavras em linguagem gestual.

**Passo 1: Colectar Sequ√™ncias de Palavras**

1. Executa a aplica√ß√£o de reconhecimento de palavras:
   ```bash
   python src/main/word_recognition/word_recognition_app.py
   ```
2. Seleciona **"1 - Coletar Sequ√™ncias"**
3. Escolhe uma palavra da lista ou cria uma nova
4. Posiciona-te na frente da c√¢mara
5. Pressiona **ESPA√áO** para come√ßar a gravar uma sequ√™ncia
6. Executa os gestos que formam a palavra completa (30 frames por sequ√™ncia)
7. Coleciona m√∫ltiplas amostras da mesma palavra (m√≠nimo 2, recomenda-se 20-30)
8. Repete para diferentes palavras

**Passo 2: Treinar o Modelo LSTM**

1. No menu principal, seleciona **"2 - Treinar Modelo"**
2. O sistema ir√°:
   - Carregar sequ√™ncias de `src/main/word_recognition/data/`
   - Verificar se h√° amostras suficientes por palavra (m√≠nimo 2)
   - Treinar um modelo LSTM sequencial com 3 camadas
   - Utilizar divis√£o estratificada (80% treino, 20% teste)
   - Aplicar callbacks de Early Stopping
   - Guardar o modelo em `src/main/word_recognition/models/`
   - Gerar relat√≥rio de classifica√ß√£o e m√©tricas

#### üë§ 3. Reconhecimento Facial (`src/utils/face-recon/`)

Este m√≥dulo permite treinar o sistema para reconhecer faces espec√≠ficas utilizando features extra√≠das por MobileNetV2.

**M√©todo de Treino (Captura em Tempo Real):**

1. Executa o m√≥dulo de reconhecimento facial:
   ```bash
   python src/utils/face-recon/face.py
   ```
2. Pressiona **'t'** para ativar o modo de treino
3. Introduz o nome da pessoa que queres registar
4. Posiciona a face na √°rea de dete√ß√£o (ret√¢ngulo verde)
5. Pressiona **'c'** para capturar e guardar a face
6. O sistema ir√°:
   - Detetar a face usando MediaPipe
   - Extrair a regi√£o facial
   - Redimensionar para 224√ó224 pixels
   - Extrair features usando MobileNetV2 pr√©-treinada
   - Guardar as features em `known_faces_data.npz`
   - Guardar imagem de refer√™ncia em `ref_faces/`

### üîß Models

- Gestos: `models/custom/custom_gesture_model.keras`
- Palavras: `src/main/word_recognition/models/word_recognition_model.keras`
- Faces: `models/custom/known_faces_data.npz`

### üìÅ Estrutura do Projeto

```
STRG/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ # Assets para o GitHub (logo, demo.gif).
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # Reconhecimento de Gestos.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gesture_recognizer.task     # Modelo pr√©-treinado MediaPipe.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ word_recognition/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ word_recognition_app.py # Reconhecimento de Palavras.
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ HandTrackingModule.py       # M√≥dulo partilhado para dete√ß√£o de m√£os.
‚îÇ       ‚îú‚îÄ‚îÄ 3d-hand-viewer/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ python/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ hand_detection.py   # Visualizador 3D da m√£o.
‚îÇ       ‚îú‚îÄ‚îÄ binary-vision/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ binary_vision.py        # Vis√£o bin√°ria.
‚îÇ       ‚îú‚îÄ‚îÄ face-recon/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ face.py                 # Reconhecimento facial.
‚îÇ       ‚îú‚îÄ‚îÄ menus/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ performance-menu/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ performance_menu.py # Menu de performance.
‚îÇ       ‚îú‚îÄ‚îÄ mouse-control-hand/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ mouse_control.py        # Controlo do cursor.
‚îÇ       ‚îî‚îÄ‚îÄ volume-control-hand/
‚îÇ           ‚îî‚îÄ‚îÄ main.py                 # Controlo do volume.
‚îú‚îÄ‚îÄ .gitattributes
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ LSTM-explicada.html     # Visualiza√ß√£o interativa do modelo LSTM.
‚îú‚îÄ‚îÄ readme.md
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python.
‚îî‚îÄ‚îÄ STRG.py                 # Launcher principal.
```

## üôè Agradecimentos

- O desenvolvimento da componente de visualiza√ß√£o 3D da m√£o foi conceptualmente inspirado pelo trabalho de Florian Rival no projeto [virtual-hand-clone](https://github.com/trflorian/virtual-hand-clone).
- A visualiza√ß√£o LSTM foi inspirada nos artigos [Neural Networks: Representation](https://www.jeremyjordan.me/intro-to-neural-networks/) de Jeremy Jordan e [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.<br>

- Outras inspira√ß√µes: <br>
  - [hand-gesture-recognition-mediapipe by kinivi](https://github.com/kinivi/hand-gesture-recognition-mediapipe); <br>
  - [colah.github.io](https://colah.github.io/);

## üìÑ Licen√ßa

Este projeto est√° licenciado sob os termos da [Licen√ßa MIT](LICENSE).
